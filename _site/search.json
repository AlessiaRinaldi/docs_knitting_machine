[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automated Knitting Machine",
    "section": "",
    "text": "The idea behind this project draws inspiration from the operating principle of 3D printers: starting from a digital model, they can automatically produce objects with precision and repeatability.\nThe objective of this work is to bring the same paradigm to textiles, by developing an automated knitting machine capable of producing knit patterns from a predefined digital design."
  },
  {
    "objectID": "components.html",
    "href": "components.html",
    "title": "Components",
    "section": "",
    "text": "Key Components\n\nMotors, drivers, sensors\nCamera & lighting\nMCU & wiring"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "control-system.html",
    "href": "control-system.html",
    "title": "Control System",
    "section": "",
    "text": "Overview\n\nReal-time control loops\nVision-based thread detection\nSafety & states"
  },
  {
    "objectID": "error-recognition.html",
    "href": "error-recognition.html",
    "title": "Error Recognition",
    "section": "",
    "text": "Textile practices such as crochet and knitting have recently experienced a widespread resurgence. Many creative activities traditionally rooted in manual craftsmanship, once considered less relevant in the context of industrialization and globalization, are now being rediscovered. Skills that historically required years of hands-on experience can, however, be difficult and discouraging for beginners, especially for adults who approach these practices with limited time, resources, or access to expert guidance. Crochet is a notable example: although widely practiced across cultures and historically spread in Europe through Italian convents in the sixteenth century, it remains technically demanding and highly dependent on the maker‚Äôs ability to recognize and correct errors. The COVID-19 pandemic further contributed to this renewed interest, encouraging many people to explore slow, creative, and tactile activities.\nDespite this revival, crochet performed using mechanical devices has seen relatively little technological development. Existing crochet machines remain strongly operator-dependent and offer limited support for automated quality monitoring. Moreover, hobbyist tools such as circular crochet looms, widely available and inexpensive, provide no error feedback and require constant attention from the user. In this work, we start from a commercially available circular loom intended for hobby use and, through a series of mechanical, electronic, and software modifications, transform it into a fully automated crochet machine capable of self-monitoring. This approach demonstrates how accessible off-the-shelf tools can be augmented and repurposed into automated systems, lowering the barrier to entry for creative technologies.\nA single undetected defect, such as missing yarn, incomplete engagement, or a misaligned loop, may propagate across many subsequent cycles, compromising the entire production and resulting in wasted time and materials. For hobbyists and individual makers, this leads to frustration and loss of work. For small and medium-sized factories, the consequences extend to production inefficiencies and environmental impact. As the textile industry is one of the major contributors to global waste and pollution, improving error detection and reducing material waste is a relevant challenge from both practical and sustainability perspectives.\nAutomating the early detection of such defects requires monitoring the hook at the precise moment when the yarn configuration becomes informative. In the scenario addressed in this work, a correctly formed stitch appears as a characteristic ‚ÄúU‚Äù-shaped engagement around the base of the hook. Any deviation from this expected pattern must be identified promptly to prevent the defect from propagating downstream. A hardware microswitch mounted on the machine provides a synchronization signal marking the exact phase of the hook movement, enabling image acquisition at each cycle for real-time inspection.\nIn this section, we present a lightweight, real-time embedded vision system for hook-level error detection implemented entirely on a Raspberry Pi 3 equipped with a CSI camera module and mounted on a custom 3D-printed support. The system captures images in synchrony with the machine‚Äôs motion, isolates the region of interest, and analyzes it using a dedicated detection algorithm capable of distinguishing correct yarn engagement from error conditions. Our goal is to support both individual makers, who may not have access to industrial-grade equipment, and future industrial applications, where inline quality monitoring could significantly reduce waste and promote more sustainable textile manufacturing."
  },
  {
    "objectID": "error-recognition.html#error-taxonomy",
    "href": "error-recognition.html#error-taxonomy",
    "title": "Error Recognition",
    "section": "",
    "text": "Missing thread: no yarn present during a stitch.\nThread break: yarn snapped or slack beyond tolerance.\nMis-stitch / skipped stitch: hook failed to catch/transfer the loop.\nJams / obstruction: mechanical interference preventing motion.\nBacklash / positioning drift: kinematic error causing misplaced stitches.\nLighting drift: exposure or color shift degrading vision accuracy."
  },
  {
    "objectID": "error-recognition.html#detection-pipeline-high-level",
    "href": "error-recognition.html#detection-pipeline-high-level",
    "title": "Error Recognition",
    "section": "",
    "text": "Sensing: camera frames + optional encoders / tension sensor.\nPreprocessing: crop, denoise, exposure normalization.\nInference: lightweight CNN / classical vision (edges, morphology).\nDecision logic: thresholds, temporal filtering, hysteresis.\nAction: warn, pause, retry, or safe-stop with guided recovery."
  },
  {
    "objectID": "error-recognition.html#dataset-labeling",
    "href": "error-recognition.html#dataset-labeling",
    "title": "Error Recognition",
    "section": "",
    "text": "Classes: {ok, missing_thread, break, mis_stitch, jam}.\nLabeling protocol: annotate bounding boxes/masks around hook & loop region; store frame index + machine state.\nSplit: train/val/test with subject and lighting stratification.\nAugmentations: brightness/contrast jitter, small blur, slight rotations."
  },
  {
    "objectID": "error-recognition.html#metrics",
    "href": "error-recognition.html#metrics",
    "title": "Error Recognition",
    "section": "",
    "text": "Per-class F1 and macro-F1 as primary.\nLatency (ms/frame), throughput (fps), false alarm rate (%/hr).\nMTTD (mean time to detection) for critical faults."
  },
  {
    "objectID": "error-recognition.html#runtime-deployment",
    "href": "error-recognition.html#runtime-deployment",
    "title": "Error Recognition",
    "section": "",
    "text": "Target: ‚â• 25‚Äì30 fps @ 320‚Äì640 px ROI.\nBudget: ‚â§ 25 ms inference, ‚â§ 10 ms I/O, ‚â§ 5 ms post-proc.\nFail-safe: if confidence &lt; threshold for N frames ‚Üí degrade to safe mode."
  },
  {
    "objectID": "error-recognition.html#requirements",
    "href": "error-recognition.html#requirements",
    "title": "Error Recognition",
    "section": "Requirements",
    "text": "Requirements\n\nShutter: global preferred (avoid motion artifacts on fast hooks).\nFrame rate: ‚â• 60 fps recommended; ‚â• 120 fps if fast cycles.\nResolution: enough to resolve the loop region (typically 320‚Äì800 px ROI on the hook).\nInterface: CSI (low latency) or USB3 (ubiquitous). GigE only if long cable runs.\nLens: C/CS-mount or M12 with suitable focal length; low distortion.\nLighting: diffuse LED ring or backlight; fixed CCT; flicker-free.\nSync: exposure synced to machine phase (encoder or GPIO trigger) if possible.\nMounting: rigid, vibration-damped, easy to re-calibrate."
  },
  {
    "objectID": "error-recognition.html#candidate-options-examples",
    "href": "error-recognition.html#candidate-options-examples",
    "title": "Error Recognition",
    "section": "Candidate Options (Examples)",
    "text": "Candidate Options (Examples)\n\nRaspberry Pi Camera (HQ / v3): CSI, low latency, good ecosystem; pick global shutter variant for fast motion.\nUSB3 industrial (e.g., IMX sensors): robust drivers, global shutter options, flexible lenses; slightly higher cost.\nLow-cost USB webcams: easy, but rolling shutter + auto-exposure can hurt reliability."
  },
  {
    "objectID": "error-recognition.html#decision-matrix-template",
    "href": "error-recognition.html#decision-matrix-template",
    "title": "Error Recognition",
    "section": "Decision Matrix (Template)",
    "text": "Decision Matrix (Template)\n\n\n\n\n\n\n\n\n\n\nCriterion\nWeight\nPi Cam (GS)\nUSB3 Industrial\nUSB Webcam\n\n\n\n\nLatency / Sync\n0.25\n4\n4\n2\n\n\nMotion artifacts (GS)\n0.25\n5\n5\n2\n\n\nImage quality / optics\n0.20\n4\n5\n3\n\n\nCost\n0.15\n4\n3\n5\n\n\nEase of integration\n0.15\n4\n4\n4\n\n\nWeighted score\n\n4.3\n4.4\n3.1\n\n\n\nScoring 1‚Äì5. Adjust weights to your constraints."
  },
  {
    "objectID": "error-recognition.html#recommended-baseline",
    "href": "error-recognition.html#recommended-baseline",
    "title": "Error Recognition",
    "section": "Recommended Baseline",
    "text": "Recommended Baseline\n\nGlobal-shutter camera (CSI or USB3) + fixed-focus lens covering the hook area + diffuse LED lighting.\nLock exposure/white balance; disable auto features.\nOptional GPIO trigger to align exposure with hook phase."
  },
  {
    "objectID": "error-recognition.html#recovery-hmi",
    "href": "error-recognition.html#recovery-hmi",
    "title": "Error Recognition",
    "section": "Recovery & HMI",
    "text": "Recovery & HMI\n\nOn detection: pause motion at safe phase, highlight ROI capture, show probable cause and guided steps (e.g., re-thread).\nOperator tools: snapshot, slow-motion replay (last 0.5‚Äì1.0 s), one-click retry."
  },
  {
    "objectID": "error-recognition.html#roadmap",
    "href": "error-recognition.html#roadmap",
    "title": "Error Recognition",
    "section": "Roadmap",
    "text": "Roadmap\n\nPhase 1: classical vision baseline (edges + region stats).\nPhase 2: lightweight CNN for robustness to yarn/lighting variants.\nPhase 3: multi-sensor fusion (tension + vision) and predictive alarms."
  },
  {
    "objectID": "error-recognition.html#choose-a-topic",
    "href": "error-recognition.html#choose-a-topic",
    "title": "Error Recognition",
    "section": "",
    "text": "üëâ Acquisition System (Camera)\nRequirements, sensor & lens selection, lighting, sync, and mounting constraints.\nüíª Error Recognition ‚Äî Code\nAlgorithms, model/code structure, data flow, and deployment notes.\nüß© 3D Camera Mount\nMechanical support, rigidity, vibration damping, working distance, and CAD notes.\nüìà Results & Efficiency\nDatasets, metrics (F1, latency, false alarms), ablation studies, and runtime budget."
  },
  {
    "objectID": "error-recognition.html#selection-of-the-acquisition-system-for-error-detection",
    "href": "error-recognition.html#selection-of-the-acquisition-system-for-error-detection",
    "title": "Error Recognition",
    "section": "- Selection of the Acquisition System for Error Detection",
    "text": "- Selection of the Acquisition System for Error Detection\nRequirements, sensor & lens selection, lighting, sync, and mounting constraints."
  },
  {
    "objectID": "error-recognition.html#error-recognition-code",
    "href": "error-recognition.html#error-recognition-code",
    "title": "Error Recognition",
    "section": "- Error Recognition ‚Äî Code",
    "text": "- Error Recognition ‚Äî Code\nAlgorithms, model/code structure, data flow, and deployment notes."
  },
  {
    "objectID": "error-recognition.html#d-camera-mount",
    "href": "error-recognition.html#d-camera-mount",
    "title": "Error Recognition",
    "section": "- 3D Camera Mount",
    "text": "- 3D Camera Mount\nMechanical support, 3D printing, working distance, and CAD notes."
  },
  {
    "objectID": "error-recognition.html#results-efficiency",
    "href": "error-recognition.html#results-efficiency",
    "title": "Error Recognition",
    "section": "- Results & Efficiency",
    "text": "- Results & Efficiency\nDatasets, metrics (F1, latency, false alarms), ablation studies, and runtime budget."
  },
  {
    "objectID": "error-camera.html",
    "href": "error-camera.html",
    "title": "Selection of the Acquisition System for Error Detection on a Raspberry Pi 3 model B+",
    "section": "",
    "text": "The goal of the system is to ensure automatic monitoring of the knitting process at each hook movement.\nA micro switch provides a synchronization signal, allowing the vision system to verify that the yarn (regardless of its color, thickness, or texture) is correctly positioned around the base of the hook.\nIn particular, the yarn should take on a ‚ÄúU‚Äù-shaped configuration embracing the base of the hook.\nAny configuration that does not match this pattern must be detected as an error.\nWhen an anomaly is detected, the system should:\n\nImmediately stop the motor driving the process;\n\nDisplay an error message and indicate the index of the corresponding hook.\n\nInsert example photo here."
  },
  {
    "objectID": "error-camera.html#objectives",
    "href": "error-camera.html#objectives",
    "title": "Selection of the Acquisition System for Error Detection on a Raspberry Pi 3 model B+",
    "section": "",
    "text": "The goal of the system is to ensure automatic monitoring of the knitting process at each hook movement.\nA micro switch provides a synchronization signal, allowing the vision system to verify that the yarn (regardless of its color, thickness, or texture) is correctly positioned around the base of the hook.\nIn particular, the yarn should take on a ‚ÄúU‚Äù-shaped configuration embracing the base of the hook.\nAny configuration that does not match this pattern must be detected as an error.\nWhen an anomaly is detected, the system should:\n\nImmediately stop the motor driving the process;\n\nDisplay an error message and indicate the index of the corresponding hook.\n\nInsert example photo here."
  },
  {
    "objectID": "error-camera.html#requirements",
    "href": "error-camera.html#requirements",
    "title": "Selection of the Acquisition System for Error Detection on a Raspberry Pi 3 model B+",
    "section": "Requirements",
    "text": "Requirements\nTo meet these objectives, the choice of camera and acquisition system must satisfy the following requirements:\n\nCompatibility with Raspberry Pi 3: use of natively supported interfaces and libraries.\n\nEase of software integration: availability of well-established computer vision libraries (e.g., libcamera, Picamera2, OpenCV).\n\nCost-effectiveness: a sensor with sufficient resolution to distinguish fine yarn details, while keeping costs low.\n\nClose-focus capability: ability to capture sharp images at short distances, near the hook area.\n\nCompact dimensions: a small camera module to facilitate mechanical integration within the machine‚Äôs 3D frame.\n\nLow power consumption: efficient power usage to minimize the overall system load."
  },
  {
    "objectID": "error-recognition.html#from-tradition-to-automation",
    "href": "error-recognition.html#from-tradition-to-automation",
    "title": "Error Recognition",
    "section": "",
    "text": "Textile practices such as crochet and knitting have recently experienced a widespread resurgence. Many creative activities traditionally rooted in manual craftsmanship, once considered less relevant in the context of industrialization and globalization, are now being rediscovered. Skills that historically required years of hands-on experience can, however, be difficult and discouraging for beginners, especially for adults who approach these practices with limited time, resources, or access to expert guidance. Crochet is a notable example: although widely practiced across cultures and historically spread in Europe through Italian convents in the sixteenth century, it remains technically demanding and highly dependent on the maker‚Äôs ability to recognize and correct errors. The COVID-19 pandemic further contributed to this renewed interest, encouraging many people to explore slow, creative, and tactile activities.\nDespite this revival, crochet performed using mechanical devices has seen relatively little technological development. Existing crochet machines remain strongly operator-dependent and offer limited support for automated quality monitoring. Moreover, hobbyist tools such as circular crochet looms, widely available and inexpensive, provide no error feedback and require constant attention from the user. In this work, we start from a commercially available circular loom intended for hobby use and, through a series of mechanical, electronic, and software modifications, transform it into a fully automated crochet machine capable of self-monitoring. This approach demonstrates how accessible off-the-shelf tools can be augmented and repurposed into automated systems, lowering the barrier to entry for creative technologies.\nA single undetected defect, such as missing yarn, incomplete engagement, or a misaligned loop, may propagate across many subsequent cycles, compromising the entire production and resulting in wasted time and materials. For hobbyists and individual makers, this leads to frustration and loss of work. For small and medium-sized factories, the consequences extend to production inefficiencies and environmental impact. As the textile industry is one of the major contributors to global waste and pollution, improving error detection and reducing material waste is a relevant challenge from both practical and sustainability perspectives.\nAutomating the early detection of such defects requires monitoring the hook at the precise moment when the yarn configuration becomes informative. In the scenario addressed in this work, a correctly formed stitch appears as a characteristic ‚ÄúU‚Äù-shaped engagement around the base of the hook. Any deviation from this expected pattern must be identified promptly to prevent the defect from propagating downstream. A hardware microswitch mounted on the machine provides a synchronization signal marking the exact phase of the hook movement, enabling image acquisition at each cycle for real-time inspection.\nIn this section, we present a lightweight, real-time embedded vision system for hook-level error detection implemented entirely on a Raspberry Pi 3 equipped with a CSI camera module and mounted on a custom 3D-printed support. The system captures images in synchrony with the machine‚Äôs motion, isolates the region of interest, and analyzes it using a dedicated detection algorithm capable of distinguishing correct yarn engagement from error conditions. Our goal is to support both individual makers, who may not have access to industrial-grade equipment, and future industrial applications, where inline quality monitoring could significantly reduce waste and promote more sustainable textile manufacturing."
  },
  {
    "objectID": "index.html#selection-of-the-acquisition-system-for-error-detection",
    "href": "index.html#selection-of-the-acquisition-system-for-error-detection",
    "title": "Automated Knitting Machine",
    "section": "- Selection of the Acquisition System for Error Detection",
    "text": "- Selection of the Acquisition System for Error Detection\nRequirements, sensor & lens selection, lighting, sync, and mounting constraints."
  },
  {
    "objectID": "index.html#error-recognition-code",
    "href": "index.html#error-recognition-code",
    "title": "Automated Knitting Machine",
    "section": "- Error Recognition ‚Äî Code",
    "text": "- Error Recognition ‚Äî Code\nAlgorithms, model/code structure, data flow, and deployment notes."
  },
  {
    "objectID": "index.html#d-camera-mount",
    "href": "index.html#d-camera-mount",
    "title": "Automated Knitting Machine",
    "section": "- 3D Camera Mount",
    "text": "- 3D Camera Mount\nMechanical support, 3D printing, working distance, and CAD notes."
  },
  {
    "objectID": "index.html#results-efficiency",
    "href": "index.html#results-efficiency",
    "title": "Automated Knitting Machine",
    "section": "- Results & Efficiency",
    "text": "- Results & Efficiency\nDatasets, metrics (F1, latency, false alarms), ablation studies, and runtime budget."
  },
  {
    "objectID": "index.html#components",
    "href": "index.html#components",
    "title": "Automated Knitting Machine",
    "section": "- Components",
    "text": "- Components"
  },
  {
    "objectID": "index.html#errore",
    "href": "index.html#errore",
    "title": "Automated Knitting Machine",
    "section": "- Errore",
    "text": "- Errore\nRequirements, sensor & lens selection, lighting, sync, and mounting constraints."
  },
  {
    "objectID": "index.html#error-recognition",
    "href": "index.html#error-recognition",
    "title": "Automated Knitting Machine",
    "section": "- Error recognition",
    "text": "- Error recognition"
  },
  {
    "objectID": "index.html#control-s",
    "href": "index.html#control-s",
    "title": "Automated Knitting Machine",
    "section": "- Control S",
    "text": "- Control S\nMechanical support, 3D printing, working distance, and CAD notes."
  },
  {
    "objectID": "index.html#control-system",
    "href": "index.html#control-system",
    "title": "Automated Knitting Machine",
    "section": "- Control System",
    "text": "- Control System"
  },
  {
    "objectID": "error-camera.html#considered-architectures",
    "href": "error-camera.html#considered-architectures",
    "title": "Selection of the Acquisition System for Error Detection on a Raspberry Pi 3 model B+",
    "section": "Considered Architectures",
    "text": "Considered Architectures\nThis section describes the image acquisition options evaluated for the error detection system on Raspberry Pi 3, highlighting their implications in terms of image quality, latency, software integration, and overall system complexity.\n\nPi Camera via CSI Interface (recommended choice)\nThe Raspberry Pi Camera v2.1 connects to the Broadcom SoC through the MIPI-CSI interface, a high-speed differential serial bus dedicated to video capture.\nThis direct connection offers several advantages:\n\nHigh bandwidth and low latency: the pixel stream reaches the processor without passing through the USB or network stack, resulting in lower jitter and more deterministic timing.\n\nSuperior image quality: 8 MP sensor with the Raspberry platform‚Äôs integrated ISP, providing fine control over exposure, gain, white balance, and frame rate.\n\nNative integration: direct support via libcamera/Picamera2 and V4L2, with robust pipelines for OpenCV, TFLite, and other vision frameworks.\n\nImplications: a better signal-to-noise ratio and frame consistency lead to improved accuracy in detecting subtle defects.\nThe low latency facilitates synchronized triggering with the microswitch event.\n\n\n\nESP32-CAM via Wi-Fi or Serial\nThe ESP32-CAM integrates an OV2640 sensor (2 MP) and a microcontroller with Wi-Fi connectivity.\nTypical acquisition involves onboard JPEG/MJPEG compression and frame transmission:\n\nWi-Fi: sends frames or snapshots to a broker/server; wiring is simple, but latency and jitter depend on network conditions and buffering.\n\nSerial/USB-UART: sends data in chunks to the Raspberry Pi. It reduces network uncertainty but requires fragmentation/reassembly protocols and buffer management.\n\nImplications: extremely low cost and stand-alone operation, but lower image quality and controllability compared to CSI.\nCompression may hide fine defects, and the less deterministic timing makes it harder to align precisely with the hook‚Äôs motion.\nMoreover, the ESP32-CAM module is bulkier than the Pi Camera, making integration into the existing mechanical frame more difficult.\nOn the positive side, its shorter minimum focus distance makes it potentially more suitable for close-up shots without additional lenses.\n\nSerial Variant Experiment (unsuccessful)\nIn preliminary tests with a direct USB‚ÄìUART connection, the module was successfully flashed with a MicroPython firmware, but upon reboot, the board remained stuck in the bootloader. The serial output only produced unreadable characters or repetitive error messages, never reaching the REPL prompt. This prevented validation of direct communication with the Raspberry Pi.\n\n\nMQTT Variant Experiment (unsuccessful)\nWithin the Wi-Fi setup, a connection from ESP32-CAM ‚Üí Raspberry Pi was attempted via an MQTT broker, following a snapshot-based approach:\n\nThe ESP32-CAM captures a JPEG frame every few seconds and publishes it to an MQTT topic.\n\nThe Raspberry Pi, running a Mosquitto broker, subscribes to the topic, decodes the images, and displays/saves them locally.\n\nDespite the theoretical simplicity of this architecture, the practical tests were unsuccessful.\nIn particular, transmission and decoding issues occurred with the JPEG payloads (published directly via MQTT), making it impossible to correctly reconstruct the images on the Raspberry Pi side.\nGiven these early failures and adopting an agile development approach (iterative and adaptive, as opposed to a waterfall model), the decision was made not to continue integrating the ESP32-CAM through this method, favoring more reliable solutions consistent with the project requirements.\nConclusion: the ESP32-CAM via MQTT approach proved too unreliable for real-time error detection, further reinforcing the preference for the Pi Camera connected via CSI.\n\n\n\n\nOther Considered Alternatives (not tested)\n\nUSB Webcam\nA ready-to-use and widely available solution with UVC support.\nHowever, the data path goes through the USB stack, adding overhead compared to CSI and limiting camera control.\nIt can be suitable for prototyping or when latency requirements are less strict.\n\n\nRaspberry Pi HQ Camera\nA 12 MP module with a C/CS mount for interchangeable lenses.\nIt offers superior optical quality and flexibility (focus adjustment, focal lengths, dedicated macro optics) at the cost of higher price and larger size. Recommended when extreme close-ups or professional optical control are required."
  },
  {
    "objectID": "error-camera.html#error-recognition-code",
    "href": "error-camera.html#error-recognition-code",
    "title": "Selection of the Acquisition System for Error Detection on a Raspberry Pi 3 model B+",
    "section": "- Error Recognition ‚Äî Code",
    "text": "- Error Recognition ‚Äî Code\nAlgorithms, model/code structure, data flow, and deployment notes."
  },
  {
    "objectID": "error-camera.html#d-camera-mount",
    "href": "error-camera.html#d-camera-mount",
    "title": "Selection of the Acquisition System for Error Detection on a Raspberry Pi 3 model B+",
    "section": "- 3D Camera Mount",
    "text": "- 3D Camera Mount\nMechanical support, 3D printing, working distance, and CAD notes."
  },
  {
    "objectID": "error-camera.html#results-efficiency",
    "href": "error-camera.html#results-efficiency",
    "title": "Selection of the Acquisition System for Error Detection on a Raspberry Pi 3 model B+",
    "section": "- Results & Efficiency",
    "text": "- Results & Efficiency\nDatasets, metrics (F1, latency, false alarms), ablation studies, and runtime budget."
  },
  {
    "objectID": "error-camera.html#results-efficiency-1",
    "href": "error-camera.html#results-efficiency-1",
    "title": "Selection of the Acquisition System for Error Detection on a Raspberry Pi 3 model B+",
    "section": "- Results & Efficiency",
    "text": "- Results & Efficiency\nDatasets, metrics (F1, latency, false alarms), ablation studies, and runtime budget."
  },
  {
    "objectID": "error-camera.html#o",
    "href": "error-camera.html#o",
    "title": "Selection of the Acquisition System for Error Detection on a Raspberry Pi 3 model B+",
    "section": "- O",
    "text": "- O\nDatasets, metrics (F1, latency, false alarms), ablation studies, and runtime budget."
  },
  {
    "objectID": "error-camera.html#over",
    "href": "error-camera.html#over",
    "title": "Selection of the Acquisition System for Error Detection on a Raspberry Pi 3 model B+",
    "section": "- Over",
    "text": "- Over\nDatasets, metrics (F1, latency, false alarms), ablation studies, and runtime budget."
  },
  {
    "objectID": "error-camera.html#overview",
    "href": "error-camera.html#overview",
    "title": "Selection of the Acquisition System for Error Detection on a Raspberry Pi 3 model B+",
    "section": "- Overview",
    "text": "- Overview"
  },
  {
    "objectID": "error-mount.html",
    "href": "error-mount.html",
    "title": "",
    "section": "",
    "text": "The camera mounting solution combines a commercially available magnetic-base stand with a custom 3D-printed support designed specifically for the Raspberry Pi Camera module.\nThe magnetic base provides a rigid and highly stable foundation, while allowing fine manual adjustment in height, angle, and lateral position through its articulated arm.\nA custom holder for the Raspberry Pi Camera v2.1 was designed in Autodesk Fusion 360 and fabricated using fused deposition modeling (FDM) with PLA filament on a Bambu Lab A1 Mini 3D printer.\nThe printed part encloses the camera PCB and incorporates a series of mounting holes and a pin that fits into the threaded end of the magnetic base arm.\nWhile the magnetic stand and the 3D-printed camera holder provide sufficient stability for prototyping, the overall stiffness of the structure is not optimal yet.\nSmall shifts caused by machine vibration or accidental contact can slightly alter the camera pose over time, affecting the exact position of the region of interest in the image.\nImproving the mechanical stability of the mounting solution is therefore an important direction for future work, with the goal of achieving fully repeatable alignment over long acquisition periods.\nA CAD rendering of the printed holder is shown in Figure.\n\n\n\n\n\nFront of the 3D structure\n\n\n\n\n\n\nBack of the 3D structure"
  },
  {
    "objectID": "error-mount.html#sec:camera_mount",
    "href": "error-mount.html#sec:camera_mount",
    "title": "",
    "section": "",
    "text": "The camera mounting solution combines a commercially available magnetic-base stand with a custom 3D-printed support designed specifically for the Raspberry Pi Camera module.\nThe magnetic base provides a rigid and highly stable foundation, while allowing fine manual adjustment in height, angle, and lateral position through its articulated arm.\nA custom holder for the Raspberry Pi Camera v2.1 was designed in Autodesk Fusion 360 and fabricated using fused deposition modeling (FDM) with PLA filament on a Bambu Lab A1 Mini 3D printer.\nThe printed part encloses the camera PCB and incorporates a series of mounting holes and a pin that fits into the threaded end of the magnetic base arm.\nWhile the magnetic stand and the 3D-printed camera holder provide sufficient stability for prototyping, the overall stiffness of the structure is not optimal yet.\nSmall shifts caused by machine vibration or accidental contact can slightly alter the camera pose over time, affecting the exact position of the region of interest in the image.\nImproving the mechanical stability of the mounting solution is therefore an important direction for future work, with the goal of achieving fully repeatable alignment over long acquisition periods.\nA CAD rendering of the printed holder is shown in Figure.\n\n\n\n\n\nFront of the 3D structure\n\n\n\n\n\n\nBack of the 3D structure"
  },
  {
    "objectID": "error-mount.html#error-recognition-code",
    "href": "error-mount.html#error-recognition-code",
    "title": "",
    "section": "- Error Recognition ‚Äî Code",
    "text": "- Error Recognition ‚Äî Code\nAlgorithms, model/code structure, data flow, and deployment notes."
  },
  {
    "objectID": "error-mount.html#d-camera-mount",
    "href": "error-mount.html#d-camera-mount",
    "title": "",
    "section": "- 3D Camera Mount",
    "text": "- 3D Camera Mount\nMechanical support, 3D printing, working distance, and CAD notes."
  },
  {
    "objectID": "error-mount.html#results-efficiency",
    "href": "error-mount.html#results-efficiency",
    "title": "",
    "section": "- Results & Efficiency",
    "text": "- Results & Efficiency\nDatasets, metrics (F1, latency, false alarms), ablation studies, and runtime budget."
  },
  {
    "objectID": "error-mount.html#overview",
    "href": "error-mount.html#overview",
    "title": "",
    "section": "- Overview",
    "text": "- Overview"
  },
  {
    "objectID": "error-mount.html#selection-of-the-acquisition-system-for-error-detection",
    "href": "error-mount.html#selection-of-the-acquisition-system-for-error-detection",
    "title": "",
    "section": "- Selection of the Acquisition System for Error Detection",
    "text": "- Selection of the Acquisition System for Error Detection\nRequirements, sensor & lens selection, lighting, sync, and mounting constraints."
  },
  {
    "objectID": "index.html#from-a-digital-design-to-fabric",
    "href": "index.html#from-a-digital-design-to-fabric",
    "title": "Automated Knitting Machine",
    "section": "",
    "text": "The idea behind this project draws inspiration from the operating principle of 3D printers: starting from a digital model, they can automatically produce objects with precision and repeatability.\nThe objective of this work is to bring the same paradigm to textiles, by developing an automated knitting machine capable of producing knit patterns from a predefined digital design."
  },
  {
    "objectID": "index.html#project-sections",
    "href": "index.html#project-sections",
    "title": "Automated Knitting Machine",
    "section": "Project Sections",
    "text": "Project Sections\n\n\n\n\n\n\n\n\nNoteError recognition\n\n\n\nReal-time detection of defects and anomalies during operation.\nOpen ‚Üí\n\n\n\n\n\n\n\n\n\nNoteComponents\n\n\n\nMechanical, electronic, and sensing elements of the platform.\nOpen ‚Üí\n\n\n\n\n\n\n\n\n\nNoteControl System\n\n\n\nActuation, logic, and synchronization of the machine.\nOpen ‚Üí"
  }
]